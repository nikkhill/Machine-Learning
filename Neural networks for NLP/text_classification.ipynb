{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFPxHyLgLh71",
    "colab_type": "text"
   },
   "source": [
    "# Download dataset, build vocabulary and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JRClj5Fmm8dW",
    "colab_type": "code",
    "outputId": "a2e13df2-a5c2-4ef9-bc73-a57ba1b838ae",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-02-11 14:14:47--  http://phontron.com/data/topicclass-v1.tar.gz\n",
      "Resolving phontron.com (phontron.com)... 208.113.196.149\n",
      "Connecting to phontron.com (phontron.com)|208.113.196.149|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15665160 (15M) [application/x-tar]\n",
      "Saving to: ‘topicclass-v1.tar.gz’\n",
      "\n",
      "topicclass-v1.tar.g 100%[===================>]  14.94M  45.7MB/s    in 0.3s    \n",
      "\n",
      "2019-02-11 14:14:48 (45.7 MB/s) - ‘topicclass-v1.tar.gz’ saved [15665160/15665160]\n",
      "\n",
      "topicclass/\n",
      "topicclass/topicclass_valid.txt\n",
      "topicclass/topicclass_test.txt\n",
      "topicclass/topicclass_train.txt\n"
     ]
    }
   ],
   "source": [
    "!wget http://phontron.com/data/topicclass-v1.tar.gz\n",
    "!tar -xvf topicclass-v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "CZ3Pu6GunK52",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "jFuTyfqFnOsz",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class Voc:\n",
    "    def __init__(self):\n",
    "        self.w2i = defaultdict(lambda: len(self.w2i))\n",
    "        self.wcounts = defaultdict(lambda: 0)\n",
    "        self.PAD = self.w2i[\"<pad>\"]\n",
    "        self.UNK = self.w2i[\"<unk>\"]\n",
    "        self.char2i = defaultdict(lambda: len(self.char2i))\n",
    "        self.PADCHAR = self.char2i[\"<pad>\"]\n",
    "        self.UNKCHAR = self.char2i[\"<unk>\"]\n",
    "#         [self.char2i[c] for c in 'abcdefghijklmnopqrstuvwxyz,.?-!'] # assumes all lowercase\n",
    "#         self.char2i = defaultdict(lambda: self.UNKCHAR, self.char2i)\n",
    "        self.char_vocab_len = len(self.char2i)\n",
    "        \n",
    "    def add_sentence(self, line):\n",
    "        # returns list of indices\n",
    "        line = line.strip()\n",
    "        words = []\n",
    "        for w in line.split():\n",
    "            words.append(self.w2i[w])\n",
    "            [self.char2i[c] for c in w]\n",
    "            self.wcounts[w] += 1\n",
    "        return words\n",
    "    \n",
    "    def return_indices(self, line):\n",
    "        line = line.strip()\n",
    "        words = []\n",
    "        words_char_level = []\n",
    "        for w in line.split():\n",
    "            if w in self.w2i:\n",
    "                words.append(self.w2i[w])\n",
    "            else:\n",
    "                words.append(self.UNK)\n",
    "            chars = []\n",
    "            for c in w:\n",
    "                if c in self.char2i:\n",
    "                    chars.append(self.char2i[c])\n",
    "                else:\n",
    "                    chars.append(self.UNKCHAR)\n",
    "            words_char_level.append(np.array(chars))\n",
    "        return np.array(words), np.array(words_char_level)\n",
    "        \n",
    "    def trim(self, min_count):\n",
    "        to_keep = []\n",
    "        for w in self.wcounts:\n",
    "            if self.wcounts[w] >= min_count:\n",
    "                to_keep.append(w)\n",
    "        self.w2i = {}\n",
    "        self.w2i = defaultdict(lambda: len(self.w2i))\n",
    "        self.wcounts_updated = defaultdict(lambda: 0)\n",
    "        self.PAD = self.w2i[\"<pad>\"]\n",
    "        self.UNK = self.w2i[\"<unk>\"]\n",
    "        for w in to_keep:\n",
    "            self.w2i[w]\n",
    "            self.wcounts_updated[w] = self.wcounts[w]\n",
    "        self.wcounts = self.wcounts_updated\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.w2i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6q0KMVljnPJ6",
    "colab_type": "code",
    "outputId": "26d1b78e-5e03-4ac2-c898-201fe891c59a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120634\n"
     ]
    }
   ],
   "source": [
    "train_file = 'topicclass/topicclass_train.txt'\n",
    "dev_file = 'topicclass/topicclass_valid.txt'\n",
    "test_file = 'topicclass/topicclass_test.txt'\n",
    "\n",
    "voc = Voc()\n",
    "\n",
    "def build_vocab(file):\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            _, line = line.lower().split('|||')\n",
    "            voc.add_sentence(line)\n",
    "\n",
    "def load_data(file):\n",
    "    out = []\n",
    "    topics = set()\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            topic, line = line.split('|||')\n",
    "            topic = topic.strip()\n",
    "            words, words_char_level = voc.return_indices(line.lower())\n",
    "            out.append((topic, words_char_level, words))\n",
    "            topics.add(topic)\n",
    "    return out, topics\n",
    "\n",
    "build_vocab(train_file)\n",
    "print(len(voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "k2lEcLbcnRcZ",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "voc.trim(5)\n",
    "len(voc)\n",
    "nwords = len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "HqQEsvhAnTPQ",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "train_data, train_topics = load_data(train_file)\n",
    "dev_data, dev_topics = load_data(dev_file)       \n",
    "all_topics = train_topics.union(dev_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "m9e7OTaj7-Oe",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# # for k in topic_to_idx:\n",
    "# for topic in all_topics:\n",
    "#     idx = topic_to_idx[topic.lower()]\n",
    "#     idx_to_topic[idx] = topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "kRl-l8tinWuh",
    "colab_type": "code",
    "outputId": "0126d28f-a061-4b31-a21a-fd3683cadfea",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302.0
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Art and architecture',\n",
       " 1: 'Natural sciences',\n",
       " 2: 'Warfare',\n",
       " 3: 'Language and literature',\n",
       " 4: 'Media and darama',\n",
       " 5: 'Mathematics',\n",
       " 6: 'Video games',\n",
       " 7: 'Philosophy and religion',\n",
       " 8: 'Social sciences and society',\n",
       " 9: 'Geography and places',\n",
       " 10: 'Miscellaneous',\n",
       " 11: 'Engineering and technology',\n",
       " 12: 'Media and drama',\n",
       " 13: 'History',\n",
       " 14: 'Sports and recreation',\n",
       " 15: 'Agriculture, food and drink',\n",
       " 16: 'Music'}"
      ]
     },
     "execution_count": 134,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_to_idx = {topic: idx for (idx, topic) in enumerate(all_topics)}\n",
    "idx_to_topic = {idx:topic for (topic, idx) in topic_to_idx.items()}\n",
    "ntags = len(topic_to_idx)\n",
    "idx_to_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Crg6MWUKnbk3",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class SentencesDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = (torch.from_numpy(self.data[idx][2]), \n",
    "                  torch.nn.utils.rnn.pad_sequence([torch.from_numpy(x) for x in self.data[idx][1]], batch_first=True)\n",
    "                  , topic_to_idx[self.data[idx][0]])\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "def pad_vect(vect, pad_len, dim):\n",
    "    # vec padded to pad in dimension dim\n",
    "    pad_amount = list(vect.shape)\n",
    "    pad_amount[dim] = pad_len - vect.size(dim)\n",
    "    padded = torch.cat([vect, torch.zeros(*pad_amount).long()], dim=dim)\n",
    "    return padded\n",
    "    \n",
    "    \n",
    "def collate_fn(batch):\n",
    "    # takes a list of (tensor, label), returns padded examples and labels\n",
    "    # find longest sentence\n",
    "    dim = 0\n",
    "    max_len = max(map(lambda x: x[0].shape[dim], batch))\n",
    "    max_word_len = max(map(lambda x: x[1].shape[1], batch))\n",
    "    batch = list(map(lambda d: (pad_vect(d[0], pad_len=max_len, dim=dim), pad_vect(pad_vect(d[1], pad_len=max_word_len, dim=1), pad_len=max_len, dim=dim), d[2]), batch))\n",
    "    ws = torch.stack(list(map(lambda x: x[0], batch)), dim=0)\n",
    "    cs = torch.stack(list(map(lambda x: x[1], batch)), dim=0)\n",
    "    ys = torch.LongTensor(list(map(lambda x: x[2], batch)))\n",
    "    return ws, cs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "rfSwGT9MDopl",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "train_samples = SentencesDataset(train_data)\n",
    "dataloader_args = dict(shuffle=True, batch_size=512, num_workers=10, pin_memory=True, collate_fn=collate_fn) if cuda \\\n",
    "                        else dict(shuffle=False, batch_size=64, collate_fn=collate_fn)\n",
    "train_loader = dataloader.DataLoader(train_samples, **dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "t7A_lcgBnjMK",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_predictions = 0.0\n",
    "    correct_predictions = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        if batch_idx % 330 == 0:\n",
    "            print(\".\", end='')\n",
    "        optimizer.zero_grad()   \n",
    "        data_words = sample[0].to(device)\n",
    "        data_chars = sample[1].to(device)\n",
    "        target = sample[2].to(device)\n",
    "\n",
    "        outputs = model(data_words, data_chars)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += target.size(0)\n",
    "        correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    acc = (correct_predictions/total_predictions) * 100.0\n",
    "    running_loss /= len(train_loader)\n",
    "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n",
    "    print('Training Accuracy: ', acc, '%')\n",
    "    return running_loss, acc\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "\n",
    "        for batch_idx, sample in enumerate(test_loader):   \n",
    "            data_words = sample[0].to(device)\n",
    "            data_chars = sample[1].to(device)\n",
    "            target = sample[2].to(device)\n",
    "            outputs = model(data_words, data_chars)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, target).detach()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Testing Loss: ', running_loss)\n",
    "        print('Testing Accuracy: ', acc, '%')\n",
    "        return running_loss, acc\n",
    "    \n",
    "def output_results(model, test_loader, criterion):\n",
    "    output = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        for batch_idx, sample in enumerate(test_loader):  \n",
    "            data_words = sample[0].to(device)\n",
    "            data_chars = sample[1].to(device)\n",
    "            outputs = model(data_words, data_chars)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            output.extend(predicted.cpu().numpy())\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQAJRcEzWs6d",
    "colab_type": "text"
   },
   "source": [
    "# Pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "gW-X-yfnXNJq",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "p0UrQ-HuV5cz",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from emb_weights import EmbWeights\n",
    "\n",
    "ew = EmbWeights(r\"glove.6B.200d.txt\")\n",
    "emb_mat = ew.create_emb_matrix(voc.w2i)\n",
    "# print(emb_mat)\n",
    "emb_mat = torch.tensor(emb_mat).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-9eG-i6MHxL",
    "colab_type": "text"
   },
   "source": [
    "# Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "A5yLdssL-IUY",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "l2_2yrMMnY4Z",
    "colab_type": "code",
    "outputId": "26bfb16d-7447-48d5-a7f2-b49e5ffa7ec3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNClassifier(\n",
      "  (char_emb): Embedding(2154, 20, padding_idx=0)\n",
      "  (emb): Embedding(39750, 200)\n",
      "  (cnn_char): Conv1d(20, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (gru_emb): GRU(232, 100, num_layers=2, bidirectional=True)\n",
      "  (conv3): Conv1d(200, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (mpool3): MaxPool1d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (dense_layer): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (projection_layer): Linear(in_features=128, out_features=17, bias=True)\n",
      "  (dropout): Dropout(p=0.2)\n",
      ")\n",
      "number of trainable parameters: 812441\n"
     ]
    }
   ],
   "source": [
    "EMB_SIZE = 200\n",
    "CHAR_EMB_SIZE = 20\n",
    "CHAR_CNN_FILTER_SIZE = 32\n",
    "RNN_EMB_SIZE = 200\n",
    "WIN_SIZE = 3\n",
    "FILTER_SIZE_1 = 64\n",
    "FILTER_SIZE_2 = 128\n",
    "FILTER_SIZE_3 = 256\n",
    "FILTER_SIZE_4 = 256\n",
    "DENSE_SIZE = 128\n",
    "\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.char_emb = nn.Embedding(len(voc.char2i), CHAR_EMB_SIZE, padding_idx=voc.PADCHAR)\n",
    "        self.emb = nn.Embedding.from_pretrained(emb_mat, freeze=True, sparse=False)\n",
    "  \n",
    "        self.cnn_char = torch.nn.Conv1d(in_channels=CHAR_EMB_SIZE, out_channels=CHAR_CNN_FILTER_SIZE, kernel_size=3,\n",
    "                                       stride=1, padding=3//2, dilation=1, groups=1, bias=True)\n",
    "    \n",
    "        self.gru_emb = torch.nn.GRU(input_size=EMB_SIZE + CHAR_CNN_FILTER_SIZE, hidden_size=RNN_EMB_SIZE//2, num_layers=2, bidirectional=True)\n",
    "  \n",
    "        self.conv3 = torch.nn.Conv1d(in_channels=RNN_EMB_SIZE, out_channels=FILTER_SIZE_3, kernel_size=WIN_SIZE,\n",
    "                                       stride=2, padding=WIN_SIZE//2, dilation=1, groups=1, bias=True)\n",
    "        self.mpool3 = nn.MaxPool1d(WIN_SIZE, padding=WIN_SIZE//2)\n",
    "        \n",
    "        self.conv4 = torch.nn.Conv1d(in_channels=FILTER_SIZE_3, out_channels=FILTER_SIZE_4, kernel_size=WIN_SIZE,\n",
    "                                       stride=2, padding=WIN_SIZE//2, dilation=1, groups=1, bias=True)\n",
    "#         self.gru = torch.nn.GRU(input_size=FILTER_SIZE_3, hidden_size=200, num_layers=2, bidirectional=True)\n",
    "        self.dense_layer = torch.nn.Linear(in_features=FILTER_SIZE_4, out_features=DENSE_SIZE, bias=True)\n",
    "        self.projection_layer = torch.nn.Linear(in_features=DENSE_SIZE, out_features=ntags, bias=True)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        \n",
    "    \n",
    "    def forward(self, words, word_chars):\n",
    "        # char:\n",
    "        batch_size, max_len, max_wlen = word_chars.shape\n",
    "        ce = self.char_emb(word_chars) # 2, 55, 11, 10\n",
    "        ce = ce.permute(0, 1, 3, 2)\n",
    "        c = self.cnn_char(ce.view(-1 , CHAR_EMB_SIZE, max_wlen))\n",
    "        c = c.view(batch_size, max_len, CHAR_CNN_FILTER_SIZE, max_wlen)\n",
    "        c = c.max(dim=3)[0]\n",
    "        c = F.relu(c)                                # c is batch x nwords x filter_size\n",
    "        c = c.permute(0, 2, 1)\n",
    "        \n",
    "        emb = self.emb(words)                        # batch x nwords x emb_size\n",
    "        emb = emb.permute(0, 2, 1)                   # batch x emb_size x nwords\n",
    "        \n",
    "        combined_emb = torch.cat((emb, c), 1)\n",
    "        combined_emb = combined_emb.permute(2, 0, 1)\n",
    "        combined_emb, _ = self.gru_emb(combined_emb) # improved embeddings, out: (seq_len, batch, num_directions * hidden_size)\n",
    "        combined_emb = combined_emb.permute(1, 2, 0) # batch x rnn_emb_size x nwords\n",
    "        h = combined_emb\n",
    "\n",
    "        h = self.conv3(h)                            # batch x num_filters x nwords\n",
    "        h = self.mpool3(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv4(h)                            # output is (batch, Channels, outseqlen)\n",
    "        h = h.max(dim=2)[0]                          # batch x num_filters\n",
    "        h = F.relu(h)\n",
    "#         h = h.permute(2, 0, 1)\n",
    "#         print(h.size())\n",
    "#         _, h = self.gru(h) # gru needs (seq_len, batch, input_size) as input. h is (num_layers * num_directions, batch, hidden_size)\n",
    "#         h = h.permute(1, 0, 2).contiguous()\n",
    "\n",
    "#         h = h.view(h.shape[0],-1)\n",
    "        h = self.dense_layer(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        out = self.projection_layer(h)               # size(out) = batch x ntags   \n",
    "        return out\n",
    "    \n",
    "    \n",
    "model = CNNClassifier()\n",
    "print(model)\n",
    "print(\"number of trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "-8wqXuc5ngY3",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "ek_6SlD1ndg4",
    "colab_type": "code",
    "outputId": "79b8292e-7792-431e-9be9-6f5dc31feedf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253909, 643)"
      ]
     },
     "execution_count": 107,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples = SentencesDataset(train_data)\n",
    "dev_samples = SentencesDataset(dev_data)\n",
    "train_dataloader_args = dict(shuffle=True, batch_size=256, pin_memory=True, collate_fn=collate_fn) if cuda \\\n",
    "                        else dict(shuffle=False, batch_size=64, collate_fn=collate_fn)\n",
    "train_loader = dataloader.DataLoader(train_samples, **train_dataloader_args)\n",
    "\n",
    "test_dataloader_args = dict(shuffle=True, batch_size=64, pin_memory=True, collate_fn=collate_fn) if cuda \\\n",
    "                        else dict(shuffle=False, batch_size=64, collate_fn=collate_fn)\n",
    "dev_loader = dataloader.DataLoader(dev_samples, **test_dataloader_args)\n",
    "# len(dev_loader)\n",
    "len(train_samples), len(dev_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "3EoUf22xnlLG",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "Train_loss = []\n",
    "Train_acc = []\n",
    "Test_loss = []\n",
    "Test_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "D6zNRBMInm3s",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "for i in range(n_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    test_loss, test_acc = test_model(model, dev_loader, criterion)\n",
    "    Train_loss.append(train_loss)\n",
    "    Test_loss.append(test_loss)\n",
    "    Test_acc.append(test_acc)\n",
    "    Train_acc.append(train_acc)\n",
    "    if len(Test_acc) > 1 and test_acc < max(Test_acc):\n",
    "        print(\"val acc decreased\")\n",
    "    else:\n",
    "        torch.save(model.state_dict(), \"test_saved_model_edit\")\n",
    "    print('='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Zhxu1qB-00xk",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"test_saved_model_edit\"))\n",
    "test_model(model, dev_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "X_P-C254ZVvg",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(), \"test_saved_model_edit1\")\n",
    "torch.save(model2.state_dict(), \"test_saved_model_edit2\")\n",
    "torch.save(model3.state_dict(), \"test_saved_model_edit3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "nfEguLu4ZgRK",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"vocw2i\", 'w') as vf:\n",
    "  json.dump(voc.w2i, vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "d4mCyT8LvwMo",
    "colab_type": "code",
    "outputId": "33a7bbe3-53a2-4c2e-f280-07e4755a69d0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Loss:  1.041890781034123\n",
      "Testing Accuracy:  83.20373250388803 %\n",
      "Testing Loss:  1.0449582934379578\n",
      "Testing Accuracy:  81.80404354587868 %\n",
      "Testing Loss:  0.9836430793458765\n",
      "Testing Accuracy:  82.42612752721618 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9836430793458765, 82.42612752721618)"
      ]
     },
     "execution_count": 180,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model1, dev_loader, criterion)\n",
    "test_model(model2, dev_loader, criterion)\n",
    "test_model(model3, dev_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ncJ00rLH5T4-",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "test_data, test_topics = load_data(test_file)  \n",
    "test_samples = SentencesDataset(test_data)\n",
    "\n",
    "predict_dataloader_args = dict(shuffle=False, batch_size=64, pin_memory=True, collate_fn=collate_fn) if cuda \\\n",
    "                        else dict(shuffle=False, batch_size=64, collate_fn=collate_fn)\n",
    "test_loader = dataloader.DataLoader(test_samples, **predict_dataloader_args)\n",
    "dev_predict_loader = dataloader.DataLoader(dev_samples,  **predict_dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "43fkm5qd_6E8",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "model = model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "XGqWqSA86HCC",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "output = output_results(model, dev_predict_loader, criterion)\n",
    "output_file = \"valid_predictions_4.txt\"\n",
    "with open(output_file, 'w') as of:\n",
    "    for i in range(len(output)):\n",
    "        of.write(\"{}\\n\".format(idx_to_topic[output[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "JgUruyYt963x",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "topic_to_idx['UNK'] = -1\n",
    "output = output_results(model, test_loader, criterion)\n",
    "output_file = \"test_predictions_4.txt\"\n",
    "with open(output_file, 'w') as of:\n",
    "    for i in range(len(output)):\n",
    "        of.write(\"{}\\n\".format(idx_to_topic[output[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "tsNlor2n-FrO",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406.0
    },
    "outputId": "b4037aa0-05b4-4fa0-c7bc-14e488c94021"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering and technology\n",
      "Media and drama\n",
      "Sports and recreation\n",
      "Sports and recreation\n",
      "History\n",
      "History\n",
      "Music\n",
      "Music\n",
      "Warfare\n",
      "Geography and places\n",
      "  697  1507 10818 test_predictions.txt\n",
      "   697  18938 101124 topicclass/topicclass_test.txt\n",
      "UNK ||| NY 93 was moved onto NY 104 and Junction Road in Cambria in the 1940s , and altered to bypass Lockport to the south on a new highway and Robinson and Dysinger roads in 1991 .\n",
      "UNK ||| It was also staged in Hartford , Connecticut in the United States in 1983 and starred John Cullum as Hitler .\n",
      "UNK ||| In 2008 , Dodd was the Australian national Grade IV para @-@ equestrian champion .\n",
      "UNK ||| He has headlined numerous pay @-@ per @-@ view events for both the WWE and UFC , including WrestleMania XIX , WrestleMania 31 , UFC 100 , and UFC 116 .\n",
      "UNK ||| Nerva became Emperor at the age of sixty @-@ five , after a lifetime of imperial service under Nero and the rulers of the Flavian dynasty .\n",
      "UNK ||| Maynilà had been Indianized since the sixth century CE and earlier .\n",
      "UNK ||| The single peaked at number 46 on the US Billboard Hot 100 and has been certified gold by the Recording Industry Association of America ( RIAA ) for shipments of 500 @,@ 000 copies .\n",
      "UNK ||| For Independiente , Arjona returns to his trademark sound after his stylistic departure for Poquita Ropa ( 2010 ) .\n",
      "UNK ||| The next year its elements took part in the Battle of Kupres and Operation Tiger aimed at lifting the Siege of Dubrovnik .\n",
      "UNK ||| Bintulu remained a fishing village until 1969 when oil and gas reserves were discovered off the coast .\n"
     ]
    }
   ],
   "source": [
    "# !head valid_predictions.txt\n",
    "# !wc valid_predictions.txt\n",
    "# !wc topicclass/topicclass_valid.txt\n",
    "\n",
    "!head test_predictions.txt\n",
    "!wc test_predictions.txt\n",
    "!wc topicclass/topicclass_test.txt\n",
    "!head topicclass/topicclass_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "KgcphWaAJT_E",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def read_result(filename):\n",
    "    out = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            label = line.strip()\n",
    "            out.append(label)\n",
    "    return out\n",
    "\n",
    "r1 = read_result(\"test_predictions_3.txt\")\n",
    "r2 = read_result(\"test_predictions_2.txt\")\n",
    "rbest = read_result(\"test_predictions.txt\")\n",
    "\n",
    "with open(\"ensemble_results_test.csv\", \"w\") as wf:\n",
    "    for i in range(len(rbest)):\n",
    "        l1, l2, lb = r1[i], r2[i], rbest[i]\n",
    "        if l1 == l2:\n",
    "            l = l1\n",
    "        else:\n",
    "            l = lb\n",
    "        wf.write(\"{}\\n\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "U-M6woFWJ_9u",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "with open(dev_file) as df:\n",
    "  with open(\"ensemble_results_valid.csv\") as rf:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    dfl = df.readlines()\n",
    "    rfl = rf.readlines()\n",
    "    for i in range(len(dfl)):\n",
    "      if rfl[i].strip() == dfl[i].split('|||')[0].strip():\n",
    "        correct += 1\n",
    "      total += 1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "U7R4edPOKvYh",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "13ba8720-84ef-4bd4-b02f-18add819327c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(538, 643, 0.8367029548989113)"
      ]
     },
     "execution_count": 178,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct, total, correct/total"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "nnnlpa1",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
